{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a631e8d2-829d-4d6e-acc5-7df453416566",
   "metadata": {},
   "source": [
    "# Lab 4 - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec901810-a43a-470d-9b3f-b9df4a2a264d",
   "metadata": {},
   "source": [
    "Το μέγεθος του `Inverted index size:  27206` του περασμένου lab.  \n",
    "Περιέχει αρκετά tokens τα οποία δεν είναι χρήσιμα ή είναι διπλότυπα λόγω παρόμοιας γραφής, κλπ.  \n",
    "Μπορεί να γίνει πολύ μικρότερο με κατάλληλο text preprocessing κατά τη δημιουργία του index. \n",
    "Μερικές βασικές τεχνικές preprocessing περιλαμβάνουν: \n",
    "* Lowercasing\n",
    "* Removing Extra Whitespaces\n",
    "* Tokenization\n",
    "* Removing Punctuations\n",
    "* Stopwords Removal\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "Δημιουργούμε μια function για κάθε στειχειώδη λειτουργία preprocessing.\n",
    "Για πολλές από τις λειτουργίες αξιοποιούμε τη βιβλιοθήκη https://www.nltk.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c6973e-8303-428d-8e1c-494444f061a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all's well that ends well\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function that turns its input to lowercase string\n",
    "'''\n",
    "def to_lowercase(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "# Test lowercasing:\n",
    "text = \"All's Well That Ends Well\"\n",
    "text = to_lowercase(text) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b9a501-a21c-4f25-93df-da4788dab9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All's Well That Ends Well\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function that removes extra white space \n",
    "'''\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "# Test remove whitespace:\n",
    "text = '''\n",
    "  \n",
    "  \n",
    "All's     Well That \n",
    "\n",
    "Ends \n",
    "\n",
    "\n",
    "Well'''\n",
    "text = remove_whitespace(text) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c8380-17d7-4cb2-aff5-9c9c0ffaedb1",
   "metadata": {},
   "source": [
    "**Αξιοποίηση της βιβλιοθήκης nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296caa20-ca33-4a09-8b4b-64a4299b1ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.1 MB/s eta 0:00:01    |█▌                              | 71 kB 869 kB/s eta 0:00:02\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "\u001b[K     |████████████████████████████████| 763 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.1.0 nltk-3.7 regex-2022.3.15 tqdm-4.64.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fae25a-14c1-4655-b210-5fcadfdfdaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', \"'s\", 'Well', 'That', 'Ends', 'Well']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #punctuation rules\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "'''\n",
    "Function that tokenises its string input\n",
    "'''\n",
    "def do_tokenise(text):\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "# Test tokenising:\n",
    "text = \"All's Well That Ends Well\"\n",
    "text = do_tokenise(text) \n",
    "print(text) #it's a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd8f93e-e6bf-4c58-8f2c-e3934c10bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 's', 'Well', 'That', 'Ends', 'Well']\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "'''\n",
    "Function that clears punctuation form list of tokens\n",
    "'''\n",
    "def rem_puctuation(tokens):\n",
    "    cleared_tokens = []\n",
    "    for token in tokens:\n",
    "        for s in string.punctuation:\n",
    "            token = token.replace(s, '')\n",
    "        if len(token):\n",
    "            cleared_tokens.append(token)\n",
    "    return cleared_tokens\n",
    "\n",
    "tokens = ['All', \"'s\", 'Well', 'That', 'Ends', 'Well', '!!!']\n",
    "tokens = rem_puctuation(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45fd2543-35c6-4259-94b7-d662a56f3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['well', 'ends', 'well']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "'''\n",
    "Function that clears stopwords form list of **lowercase** tokens\n",
    "'''\n",
    "def rem_stopwords(tokens):\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    cleared_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in en_stopwords:\n",
    "            cleared_tokens.append(token)\n",
    "    return cleared_tokens\n",
    "\n",
    "tokens = ['all', 's', 'well', 'that', 'ends', 'well']\n",
    "tokens = rem_stopwords(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66ba2437-c9c0-4549-b389-d144900c80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'alreadi', 'deliv', 'him', 'letter', 'and', 'there', 'appear', 'much', 'joy', 'in', 'him']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "'''\n",
    "Function that performs stemming of tokens list, using Porter Stemmer\n",
    "More: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "'''\n",
    "def do_stemming(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_tokens=[]\n",
    "    for token in tokens:\n",
    "        stemmed_tokens.append(porter.stem(token))\n",
    "    return stemmed_tokens\n",
    "\n",
    "#Test\n",
    "tokens = do_stemming(rem_puctuation(do_tokenise(\"I have already delivered him letters, and there appears much joy in him\")))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1699e6e-8bff-4f50-a505-e9ef9bd0e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'already', 'deliver', 'him', 'letter', 'and', 'there', 'appear', 'much', 'joy', 'in', 'him']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "from nltk.corpus import wordnet\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "'''\n",
    "Function that performs stemming of tokens list, using Porter Stemmer\n",
    "More: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "'''\n",
    "def do_lemmization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for token,tag in pos_tag(text):\n",
    "        # print(token, tag)\n",
    "        tag=tag[0].upper()\n",
    "        tag=tag_dict.get(tag, wordnet.NOUN)\n",
    "        # print(token, tag)\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token, tag))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "#Test\n",
    "tokens = do_lemmization(rem_puctuation(do_tokenise(\"I have already delivered him letters, and there appears much joy in him\")))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65342922-306d-4f60-8393-9f5bc94d5b70",
   "metadata": {},
   "source": [
    "**Δημιουργήστε ξανά το Inverted Index της περασμένης εβδομάδας, μετά από text preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca312d50-e7d5-4ef3-947f-37142f7abca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v')) \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf8877-a739-4326-a56d-cf33cdde35b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
